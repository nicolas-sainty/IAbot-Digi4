import os
import json
import numpy as np
from typing import List
import time


from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import AIMessage, HumanMessage
from dotenv import load_dotenv
from supabase import create_client

# ğŸ“Œ Charger les variables d'environnement
load_dotenv()

SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# VÃ©rification des clÃ©s essentielles
if not SUPABASE_URL or not SUPABASE_KEY or not OPENAI_API_KEY:
    raise ValueError("VÃ©rifiez que SUPABASE_URL, SUPABASE_KEY et OPENAI_API_KEY sont dÃ©finis dans votre fichier .env")

# ğŸ“Œ Initialiser Supabase et Embeddings
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

# âœ… Fonction pour rÃ©cupÃ©rer les embeddings de Supabase
def get_all_embeddings():
    """
    RÃ©cupÃ¨re les embeddings stockÃ©s dans les tables races, drivers et results.
    """
    all_embeddings = []
    all_texts = []
    all_metadata = []

    # DÃ©finir les colonnes spÃ©cifiques Ã  chaque table
    tables = {
        "races": ("id", "name"),  # "name" est le nom de la course
        "drivers": ("driver_ref", "first_name", "last_name"),  # ConcatÃ©ner first_name + last_name
        "results": ("id", "driver_id")  # Utiliser driver_id pour identifier les rÃ©sultats
    }

    for table, columns in tables.items():
        response = supabase.table(table).select(*columns, "embedding").execute()

        if "error" in response and response["error"]:
            raise Exception(f"Erreur Supabase ({table}) : {response['error']}")

        if not response.data or len(response.data) == 0:
            print(f"âš ï¸ Aucun embedding trouvÃ© dans la table {table}.")
            continue

        for row in response.data:
            try:
                embedding_vector = json.loads(row["embedding"]) if isinstance(row["embedding"], str) else row["embedding"]
                all_embeddings.append(np.array(embedding_vector, dtype=np.float32))  # Convertir en array NumPy
                
                # GÃ©nÃ©rer le texte associÃ© Ã  l'embedding
                if table == "drivers":
                    text_value = f"{row['first_name']} {row['last_name']}"  # ConcatÃ¨ne prÃ©nom + nom
                elif table == "results":
                    text_value = f"RÃ©sultat pour le pilote {row['driver_id']}"  # Associe Ã  un pilote
                else:
                    text_value = row[columns[1]]  # Prend la colonne "name" pour races

                all_texts.append(text_value)
                all_metadata.append({"table": table, "id": row[columns[0]]})
            except Exception as e:
                print(f"âš ï¸ ProblÃ¨me avec une entrÃ©e de {table}: {e}")

    print(f"âœ… {len(all_embeddings)} embeddings rÃ©cupÃ©rÃ©s depuis Supabase.")
    return all_embeddings, all_texts, all_metadata

# âœ… Charger les embeddings dans Chroma
def load_embeddings_into_chroma():
    """
    Charge les embeddings rÃ©cupÃ©rÃ©s dans une base vectorielle Chroma.
    """
    embeddings, texts, metadata = get_all_embeddings()

    if not embeddings:
        print("âŒ Aucun embedding trouvÃ©, arrÃªt du chargement.")
        return None

    # Initialiser Chroma
    vector_store = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings_model
    )

    # Ajouter les embeddings dans Chroma
    vector_store.add_texts(
        texts=texts,
        metadatas=metadata,
        embeddings=embeddings
    )

    print("âœ… Embeddings chargÃ©s dans Chroma avec succÃ¨s !")
    return vector_store

# âœ… CrÃ©er le chatbot
def create_chatbot():
    """
    CrÃ©e un chatbot LangChain basÃ© sur les embeddings stockÃ©s dans Supabase et chargÃ©s dans Chroma.
    """
    print("ğŸ”„ Chargement des embeddings depuis Supabase...")
    vector_store = load_embeddings_into_chroma()

    if not vector_store:
        raise Exception("âŒ Impossible de charger les embeddings dans Chroma.")

    print("âœ… Chatbot prÃªt Ã  l'emploi avec les embeddings rÃ©cupÃ©rÃ©s !")
    return ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY),
        retriever=vector_store.as_retriever()
    )

# âœ… RÃ©cupÃ©rer l'historique du chat depuis Supabase
def get_chat_history(chat_id: str) -> List:
    response = supabase.table("message").select("*").eq("chat_id", chat_id).order("id").execute()

    if "error" in response and response["error"]:
        raise Exception(f"Erreur Supabase : {response['error']}")

    if not response.data or len(response.data) == 0:
        print("Aucun message trouvÃ© pour ce chat_id.")
        return []

    messages = response.data
    history = []
    for message in messages:
        if message["role"] == "user":
            history.append(HumanMessage(content=message["content"]))
        elif message["role"] == "assistant":
            history.append(AIMessage(content=message["content"]))

    return history

# âœ… RÃ©cupÃ©rer le dernier message utilisateur
def get_last_user_message() -> dict:
    response = supabase.table("message").select("*").eq("role", "user").order("id", desc=True).limit(1).execute()

    if hasattr(response, "error") and response.error:
        raise Exception(f"Erreur Supabase : {response.error}")

    if not response.data or len(response.data) == 0:
        print("âš ï¸ Aucun message utilisateur trouvÃ©.")
        return None

    return response.data[0]

# âœ… Sauvegarder la rÃ©ponse du chatbot
def save_assistant_message(chat_id: str, content: str):
    """
    Sauvegarde une rÃ©ponse de l'assistant dans la base de donnÃ©es.
    """
    response = supabase.table("message").insert({
        "chat_id": chat_id,
        "role": "assistant",
        "content": content
    }).execute()

    if isinstance(response, dict) and "error" in response:
        raise Exception(f"Erreur Supabase : {response['error']}")

    if not response.data:
        raise Exception("âš ï¸ L'insertion dans la table 'message' a Ã©chouÃ©.")

    print("âœ… RÃ©ponse de l'assistant sauvegardÃ©e avec succÃ¨s.")

# âœ… Fonction principale
import time

def process_chat():
    """
    Boucle continue pour Ã©couter et rÃ©pondre aux messages des utilisateurs
    uniquement lorsque le dernier message provient d'un utilisateur.
    """
    print("ğŸ’¬ Chatbot en attente de messages... (tape 'exit' pour quitter)\n")

    # Charger les embeddings UNE SEULE FOIS
    print("ğŸ”„ Initialisation des embeddings...")
    vector_store = load_embeddings_into_chroma()
    if not vector_store:
        print("âŒ Impossible de charger les embeddings. ArrÃªt du chatbot.")
        return

    print("âœ… Chatbot prÃªt Ã  rÃ©pondre !")

    chatbot = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY),
        retriever=vector_store.as_retriever()
    )

    last_processed_message_id = None  # Stocke l'ID du dernier message traitÃ©

    while True:
        # RÃ©cupÃ©rer le dernier message utilisateur
        last_message = get_last_user_message()

        # VÃ©rifier si un message a Ã©tÃ© trouvÃ©
        if not last_message:
            print("âš ï¸ Aucun nouveau message utilisateur trouvÃ©. En attente...")
            time.sleep(2)  # Attendre 2 secondes avant de revÃ©rifier
            continue

        chat_id = last_message["chat_id"]
        user_message = last_message["content"]
        message_id = last_message["id"]  # ID unique du message

        # VÃ©rifier si c'est un nouveau message (Ã©viter de traiter deux fois le mÃªme)
        if last_processed_message_id == message_id:
            time.sleep(2)  # Attendre avant de vÃ©rifier Ã  nouveau
            continue

        # VÃ©rifier si c'est bien un message utilisateur
        if last_message["role"] != "user":
            time.sleep(2)  # Attendre avant de vÃ©rifier Ã  nouveau
            continue

        # Mettre Ã  jour le dernier message traitÃ©
        last_processed_message_id = message_id

        # VÃ©rifier si l'utilisateur veut quitter
        if user_message.lower() in ["exit", "quit", "bye"]:
            print("ğŸ‘‹ Chatbot arrÃªtÃ©.")
            break

        # Charger l'historique du chat
        history = get_chat_history(chat_id)

        # GÃ©nÃ©rer une rÃ©ponse SANS RECHARGER LES EMBEDDINGS
        response = chatbot({"question": user_message, "chat_history": history})
        bot_response = response["answer"]
        print(f"ğŸ¤– {bot_response}")

        # Sauvegarder la rÃ©ponse dans Supabase
        save_assistant_message(chat_id, bot_response)

        # Pause pour Ã©viter une boucle trop rapide
        time.sleep(2)




# âœ… ExÃ©cuter le script
if __name__ == "__main__":
    process_chat()
